{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Three \n",
    "\n",
    "The primary description of this coursework is available on the CM20252 Moodle page. This is the Jupyter notebook you must complete and submit to receive marks. This notebook adds additional detail to the coursework specification but does not repeat the information that has already been provided there. \n",
    "\n",
    "You must follow all instructions given in this notebook precisely.\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing. Remember to save your work regularly.\n",
    "\n",
    "__You will develop players for Connect-Three on a grid that is 5 columns wide and 3 rows high. An example is shown below showing a win for Player Red.__\n",
    "\n",
    "<img src=\"images/connect3.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "For your reference, below is a visual depiction of the agent-environment interface in reinforcement learning. The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n",
    "\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Below, we provide some code that will be useful for implementing parts of this interface. You are not obligated to use this code; please feel free to develop your own code from scratch. \n",
    "\n",
    "### Code details\n",
    "\n",
    "We provide a `Connect` class that you can use to simulate Connect-Three games. The following cells in this section will walk you through the basic usage of this class by playing a couple of games.\n",
    "\n",
    "We import the `connect` module and create a Connect-Three environment called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the `Connect` object will regularly print the progress of the game. This is useful for getting to know the provided code, debugging your code, or if you just want to play around. You will want to set `verbose=False` when you run hundreds of episodes to complete the marked exercises.\n",
    "\n",
    "This `Connect` environment uses the strings `'o'` and `'x'` instead of different disk colors in order to distinguish between the two players. We can specify who should start the game using the `starting_player` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has been reset.\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "import connect\n",
    "env = connect.Connect(starting_player='x', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can interact with the environment using the `act()` method. This method takes an `action` (an integer) as input and computes the response of the environment. An action is defined as the column index that a disk is dropped into. The `act()` method returns the `reward` for player `'o'` and a boolean, indicating whether the game is over (`True`) or not (`False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n",
      "reward = 0\n",
      "game_over = False\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)\n",
    "print(\"reward =\", reward)\n",
    "print(\"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `verbose=True` when we created our environment, the grid is printed each time we call the `act()` method. You probably might want to set `verbose=False` when you run Q-learning for thousands of episodes. \n",
    "\n",
    "As expected, the `reward` is 0 and no one has won the game yet (`game_over` is `False`). Let us drop another disk into the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the `Connect` environment automatically switches the\n",
    "\n",
    "The `grid` is stored as a two-dimensional `numpy` array in the `Connect` class and you can easily access it by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "current_grid = env.grid\n",
    "print(current_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the grid now appears to be \"upside down\" because `numpy` arrays are printed from \"top to bottom\".\n",
    "We can also print it the way it is printed by the Connect class by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "print(current_grid[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to put another disk in the same column with `act(action=2)`. The environment will throw an error because that column is already filled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `.available_actions` of the `Connect` class contains a `numpy` array of all not yet filled columns. This variable should help you to avoid errors like the one we have just encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(env.available_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that column index '2' is missing because this column is already filled.\n",
    "\n",
    "Let's keep on playing until some player wins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' 'o' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' ' ' ' ']\n",
      " [' ' 'x' 'o' 'o' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "reward = 0 game_over = False\n",
      "[[' ' ' ' 'x' 'o' ' ']\n",
      " [' ' 'x' 'o' 'o' ' ']\n",
      " [' ' 'x' 'x' 'o' ' ']]\n",
      "Player ' o ' has won the game!\n",
      "reward = 1 game_over = True\n"
     ]
    }
   ],
   "source": [
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over) \n",
    "reward, game_over = env.act(action=1)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=1)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)\n",
    "reward, game_over = env.act(action=3)\n",
    "print(\"reward =\", reward, \"game_over =\", game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the `reward` returned by the `act()` method is the reward for player `'o'`.\n",
    "\n",
    "You can reset the game using the `reset()` method. This method cleans the grid and makes sure that the it is the `starting_player`'s turn as defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has been reset.\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "#reward, game_over = env.act(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify existing or add new methods to the `Connect` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "**Your opponent is always the first player. Your agent is always the second player.**\n",
    "\n",
    "For your reference, the pseudo-code for Q-learning is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 6.5).\n",
    "<img src=\"images/q_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Prepare a **learning curve** following the directions below. We refer to this as Plot 1.\n",
    "\n",
    "After $n$ steps of interaction with the environment, play $m$ games with the current policy of the agent (without modifying the policy). Think of this as interrupting the agent for a period of time to test how well it has learned so far. Your plot should show the total score obtained in these $m$ games as a function of $n, 2n, 3n, … kn$. The choices of $n$ and $k$ are up to you. They should be reasonable values that demonstrate the efficiency of the learning and how well the agent learns to play the game eventually. Use $m=10$. \n",
    "\n",
    "This plot should show the mean performance of `a` agents, not the performance of a single agent. Because of the stochasticity in the environment, you will obtain two different learning curves from two different agents even though they are using exactly the same algorithm. We suggest setting `a` to 20 or higher.\n",
    "\n",
    "Present a single mean learning curve with your choice of parameters $\\epsilon$ and $\\alpha$. The plot should also show (as a baseline) the mean performance of a random agent that does not learn but chooses actions uniformly randomly from among the legal actions. Label this line “Random Agent”. \n",
    "\n",
    "Please include this plot as a static figure in the appropriate cell below. That is, compute the learning curve in the lab or at home (this may take a couple of minutes depending on your implementation) and save the figure in the same directory as your notebook. Import this figure in the appropriate answer cell under (A). You can look at the source code of this markdown cell (double click on it!) to find out how to embed figures using html. Do **not** use drag & drop to include figures; we would not be able to see them! Make sure to include the locally stored images in your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1431aa87b9e9019a4dbe6e696e0a9082",
     "grade": true,
     "grade_id": "cell-3ac2114f764e8410",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n### Write all your code for Part 1 within or above this cell. \\nimport matplotlib.pyplot as plt\\nimport random\\nimport connect\\nimport numpy as np\\n\\n#returns the maximum q value achiveable from the current state\\ndef max_next_item(q_table, grid):\\n    table = []\\n    \\n    for i in range(0, 5):\\n        #spots that are in use are assigned a very low value so they are never picked \\n        if (env.grid[2,i] == \"x\" or env.grid[2,i] == \"o\"):\\n            q_table[grid, i] = 0\\n            table += [-10000]\\n        elif ((grid, i) in q_table):\\n            table += [q_table[grid, i]]\\n        else:\\n            q_table[grid, i] = 0\\n            table += [0]\\n            \\n    maximum = max(table)\\n    return maximum\\n\\n\\n#returns the best possible move from current state\\ndef max_next(q_table, grid):\\n    table = []\\n    for i in range(0, 5):\\n        if (env.grid[2,i] == \"x\" or env.grid[2,i] == \"o\"):\\n            q_table[grid, i] = 0\\n            table += [-10000]\\n        elif ((grid, i) in q_table):\\n            table += [q_table[grid, i]]\\n        else:\\n            q_table[grid, i] = 0\\n            table += [0]\\n            \\n    maximum = max(table)\\n    max_moves = []\\n    for i in range(0, 5):\\n        if (table[i] == maximum):\\n            max_moves += [i]\\n    \\n    #if there are multiple moves with the same q value pick randomly between them to avoid bias\\n    if (len(max_moves) == 1):\\n        return max_moves[0]\\n    else:\\n        random_move = random.randint(0,len(max_moves)-1)\\n        return max_moves[random_move]\\n\\n    \\n#converts grid to a string that can be used as a dictionary index\\ndef convert_grid_to_state(current_grid):\\n    state = \"\"\\n    for i in range(0, 3):\\n        for j in range(0, 5):\\n            if (current_grid[i][j] == \"x\"):\\n                state+=\"x\"\\n            elif (current_grid[i][j] == \"o\"):\\n                state+=\"o\"\\n            else:\\n                state+=\"y\"\\n    \\n    return state\\n\\n\\ndef plot(wins, random_wins):\\n    %matplotlib inline\\n    x = np.arange(0,len(wins))\\n    x = x*400\\n    y = np.array(wins)\\n    y2 = np.array(random_wins)\\n    fig, ax1 = plt.subplots(nrows = 1, ncols = 1 )\\n    ax1.plot(x,y2, color = \"red\", label=\"random agent\")\\n    ax1.plot(x,y, color = \"blue\", label=\"learning agent\")\\n    ax1.set_xlabel(\\'n\\')\\n    ax1.set_ylabel(\\'average agent score\\')\\n    ax1.legend()\\n    plt.show()\\n\\n    \\n#decides whether to pick a random move or an optimal one\\ndef pickMove(q_table):\\n    epsilon = 0.1\\n    random_action_chance = random.randint(0,10)\\n    #find action with biggest q value\\n    move = max_next(q_table, convert_grid_to_state(env.grid))\\n\\n    if (random_action_chance <= epsilon*10):\\n        #choose random action\\n        random_move = random.randint(0,4)\\n        while (env.grid[2,move] == \"x\" or env.grid[2,move] == \"o\"):\\n            move = random.randint(0,4)\\n            \\n    return move\\n\\n#plays n games and updates q table\\ndef train_for_n_steps(q_table):\\n    n = 400\\n    alpha = 0.1\\n    moves = 0\\n    gamma = 1\\n\\n    while (moves <= n):\\n        env.reset()\\n        #radnom npc move\\n        reward, game_over = env.act(random.randint(0,4))\\n\\n        while (game_over == False):\\n            move = pickMove(q_table)\\n            grid = convert_grid_to_state(env.grid)\\n            prev = q_table[grid, move]\\n            previous_grid = grid\\n            reward, game_over = env.act(move)\\n            #random npc move\\n            if (game_over == False):\\n                #prevents npc making invalid move\\n                npc_move = random.randint(0,4)\\n                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\\n                    npc_move = random.randint(0,4)\\n                reward, game_over = env.act(npc_move)\\n\\n            #update q table for game state and action chosen\\n            update_value = prev + alpha * (reward + gamma * (max_next_item(q_table, convert_grid_to_state(env.grid)) - prev))\\n            q_table[previous_grid, move] = update_value\\n            moves = moves + 1\\n            if (moves == n):\\n                break\\n            \\n    return(q_table)\\n        \\n#play 10 games with the current q table and don\\'t change it        \\ndef play_m_games(q_table):\\n    reward = 0\\n    m = 9\\n    score = 0\\n    gamesPlayed = 0\\n    while (gamesPlayed <= m):\\n        score = score + reward\\n        env.reset()\\n        reward, game_over = env.act(random.randint(0,4))\\n        gamesPlayed = gamesPlayed + 1\\n        while (game_over == False):\\n            move = pickMove(q_table)\\n            reward, game_over = env.act(move)\\n            if (game_over == False):\\n                #prevents npc making invalid move\\n                npc_move = random.randint(0,4)\\n                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\\n                    npc_move = random.randint(0,4)\\n                reward, game_over = env.act(npc_move)\\n    \\n    score = score + reward\\n    return score\\n\\n#play 10 games making random moves for player and npc\\ndef play_m_random_games():\\n    reward = 0\\n    m = 9\\n    score = 0\\n    gamesPlayed = 0\\n    while (gamesPlayed <= m):\\n        score = score + reward\\n        env.reset()\\n        reward, game_over = env.act(random.randint(0,4))\\n        gamesPlayed = gamesPlayed + 1\\n        while (game_over == False):\\n            move = random.randint(0,4)\\n            while (env.grid[2,move] == \"x\" or env.grid[2,move] == \"o\"):\\n                move = random.randint(0,4)\\n                \\n            reward, game_over = env.act(move)\\n            \\n            if (game_over == False):\\n                npc_move = random.randint(0,4)\\n                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\\n                    npc_move = random.randint(0,4)\\n\\n                reward, game_over = env.act(npc_move)\\n                \\n    score = score + reward\\n    #print(score)\\n    return score\\n            \\n    \\n#Main code body    \\nenv = connect.Connect(starting_player=\\'x\\', verbose=False)\\nall_wins = []\\nall_random_wins = []\\n\\n#dispatch 20 agents to play randomly and 20 to play using q table \\nfor a in range(0,20):\\n    q_table = {}\\n    wins = []\\n    for k in range(0,400):\\n        q_table = train_for_n_steps(q_table)\\n        wins += [play_m_games(q_table)]\\n    all_wins += [wins]\\n    \\nfor a in range(0,20):\\n    random_wins = []\\n    for k in range(0,400):\\n        random_wins += [play_m_random_games()]\\n    all_random_wins += [random_wins]\\n\\n#find the mean of all agents\\nall_wins = np.mean(all_wins, axis=0)\\nall_random_wins = np.mean(all_random_wins, axis=0)\\n\\nplot(all_wins, all_random_wins)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "### Write all your code for Part 1 within or above this cell. \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import connect\n",
    "import numpy as np\n",
    "\n",
    "#returns the maximum q value achiveable from the current state\n",
    "def max_next_item(q_table, grid):\n",
    "    table = []\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        #spots that are in use are assigned a very low value so they are never picked \n",
    "        if (env.grid[2,i] == \"x\" or env.grid[2,i] == \"o\"):\n",
    "            q_table[grid, i] = 0\n",
    "            table += [-10000]\n",
    "        elif ((grid, i) in q_table):\n",
    "            table += [q_table[grid, i]]\n",
    "        else:\n",
    "            q_table[grid, i] = 0\n",
    "            table += [0]\n",
    "            \n",
    "    maximum = max(table)\n",
    "    return maximum\n",
    "\n",
    "\n",
    "#returns the best possible move from current state\n",
    "def max_next(q_table, grid):\n",
    "    table = []\n",
    "    for i in range(0, 5):\n",
    "        if (env.grid[2,i] == \"x\" or env.grid[2,i] == \"o\"):\n",
    "            q_table[grid, i] = 0\n",
    "            table += [-10000]\n",
    "        elif ((grid, i) in q_table):\n",
    "            table += [q_table[grid, i]]\n",
    "        else:\n",
    "            q_table[grid, i] = 0\n",
    "            table += [0]\n",
    "            \n",
    "    maximum = max(table)\n",
    "    max_moves = []\n",
    "    for i in range(0, 5):\n",
    "        if (table[i] == maximum):\n",
    "            max_moves += [i]\n",
    "    \n",
    "    #if there are multiple moves with the same q value pick randomly between them to avoid bias\n",
    "    if (len(max_moves) == 1):\n",
    "        return max_moves[0]\n",
    "    else:\n",
    "        random_move = random.randint(0,len(max_moves)-1)\n",
    "        return max_moves[random_move]\n",
    "\n",
    "    \n",
    "#converts grid to a string that can be used as a dictionary index\n",
    "def convert_grid_to_state(current_grid):\n",
    "    state = \"\"\n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 5):\n",
    "            if (current_grid[i][j] == \"x\"):\n",
    "                state+=\"x\"\n",
    "            elif (current_grid[i][j] == \"o\"):\n",
    "                state+=\"o\"\n",
    "            else:\n",
    "                state+=\"y\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def plot(wins, random_wins):\n",
    "    %matplotlib inline\n",
    "    x = np.arange(0,len(wins))\n",
    "    x = x*400\n",
    "    y = np.array(wins)\n",
    "    y2 = np.array(random_wins)\n",
    "    fig, ax1 = plt.subplots(nrows = 1, ncols = 1 )\n",
    "    ax1.plot(x,y2, color = \"red\", label=\"random agent\")\n",
    "    ax1.plot(x,y, color = \"blue\", label=\"learning agent\")\n",
    "    ax1.set_xlabel('n')\n",
    "    ax1.set_ylabel('average agent score')\n",
    "    ax1.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#decides whether to pick a random move or an optimal one\n",
    "def pickMove(q_table):\n",
    "    epsilon = 0.1\n",
    "    random_action_chance = random.randint(0,10)\n",
    "    #find action with biggest q value\n",
    "    move = max_next(q_table, convert_grid_to_state(env.grid))\n",
    "\n",
    "    if (random_action_chance <= epsilon*10):\n",
    "        #choose random action\n",
    "        random_move = random.randint(0,4)\n",
    "        while (env.grid[2,move] == \"x\" or env.grid[2,move] == \"o\"):\n",
    "            move = random.randint(0,4)\n",
    "            \n",
    "    return move\n",
    "\n",
    "#plays n games and updates q table\n",
    "def train_for_n_steps(q_table):\n",
    "    n = 400\n",
    "    alpha = 0.1\n",
    "    moves = 0\n",
    "    gamma = 1\n",
    "\n",
    "    while (moves <= n):\n",
    "        env.reset()\n",
    "        #radnom npc move\n",
    "        reward, game_over = env.act(random.randint(0,4))\n",
    "\n",
    "        while (game_over == False):\n",
    "            move = pickMove(q_table)\n",
    "            grid = convert_grid_to_state(env.grid)\n",
    "            prev = q_table[grid, move]\n",
    "            previous_grid = grid\n",
    "            reward, game_over = env.act(move)\n",
    "            #random npc move\n",
    "            if (game_over == False):\n",
    "                #prevents npc making invalid move\n",
    "                npc_move = random.randint(0,4)\n",
    "                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\n",
    "                    npc_move = random.randint(0,4)\n",
    "                reward, game_over = env.act(npc_move)\n",
    "\n",
    "            #update q table for game state and action chosen\n",
    "            update_value = prev + alpha * (reward + gamma * (max_next_item(q_table, convert_grid_to_state(env.grid)) - prev))\n",
    "            q_table[previous_grid, move] = update_value\n",
    "            moves = moves + 1\n",
    "            if (moves == n):\n",
    "                break\n",
    "            \n",
    "    return(q_table)\n",
    "        \n",
    "#play 10 games with the current q table and don't change it        \n",
    "def play_m_games(q_table):\n",
    "    reward = 0\n",
    "    m = 9\n",
    "    score = 0\n",
    "    gamesPlayed = 0\n",
    "    while (gamesPlayed <= m):\n",
    "        score = score + reward\n",
    "        env.reset()\n",
    "        reward, game_over = env.act(random.randint(0,4))\n",
    "        gamesPlayed = gamesPlayed + 1\n",
    "        while (game_over == False):\n",
    "            move = pickMove(q_table)\n",
    "            reward, game_over = env.act(move)\n",
    "            if (game_over == False):\n",
    "                #prevents npc making invalid move\n",
    "                npc_move = random.randint(0,4)\n",
    "                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\n",
    "                    npc_move = random.randint(0,4)\n",
    "                reward, game_over = env.act(npc_move)\n",
    "    \n",
    "    score = score + reward\n",
    "    return score\n",
    "\n",
    "#play 10 games making random moves for player and npc\n",
    "def play_m_random_games():\n",
    "    reward = 0\n",
    "    m = 9\n",
    "    score = 0\n",
    "    gamesPlayed = 0\n",
    "    while (gamesPlayed <= m):\n",
    "        score = score + reward\n",
    "        env.reset()\n",
    "        reward, game_over = env.act(random.randint(0,4))\n",
    "        gamesPlayed = gamesPlayed + 1\n",
    "        while (game_over == False):\n",
    "            move = random.randint(0,4)\n",
    "            while (env.grid[2,move] == \"x\" or env.grid[2,move] == \"o\"):\n",
    "                move = random.randint(0,4)\n",
    "                \n",
    "            reward, game_over = env.act(move)\n",
    "            \n",
    "            if (game_over == False):\n",
    "                npc_move = random.randint(0,4)\n",
    "                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\n",
    "                    npc_move = random.randint(0,4)\n",
    "\n",
    "                reward, game_over = env.act(npc_move)\n",
    "                \n",
    "    score = score + reward\n",
    "    #print(score)\n",
    "    return score\n",
    "            \n",
    "    \n",
    "#Main code body    \n",
    "env = connect.Connect(starting_player='x', verbose=False)\n",
    "all_wins = []\n",
    "all_random_wins = []\n",
    "\n",
    "#dispatch 20 agents to play randomly and 20 to play using q table \n",
    "for a in range(0,20):\n",
    "    q_table = {}\n",
    "    wins = []\n",
    "    for k in range(0,400):\n",
    "        q_table = train_for_n_steps(q_table)\n",
    "        wins += [play_m_games(q_table)]\n",
    "    all_wins += [wins]\n",
    "    \n",
    "for a in range(0,20):\n",
    "    random_wins = []\n",
    "    for k in range(0,400):\n",
    "        random_wins += [play_m_random_games()]\n",
    "    all_random_wins += [random_wins]\n",
    "\n",
    "#find the mean of all agents\n",
    "all_wins = np.mean(all_wins, axis=0)\n",
    "all_random_wins = np.mean(all_random_wins, axis=0)\n",
    "\n",
    "plot(all_wins, all_random_wins)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "980d73fb62fae59d610abc96121f71bc",
     "grade": true,
     "grade_id": "cell-ce1405b859519f91",
     "locked": false,
     "points": 60,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "(A) [continued} Insert your static learning curve here (Plot 1).\n",
    "\n",
    "<img src=\"final graph.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "(B) In 3 sentences or less, explain your conclusions from the plot above. How close does your (average) agent get to the best possible level of performance? How efficiently does your (average) agent learn? \n",
    "\n",
    "At its peak the agent reaches a win ratio of a little over 95%. If the best possible performance is a 100% win rate, the agent gets close being 10-5% away the majority of the time by the later episodes. It learns in an efficient manor, with the initial gradient of learning being very steep, the majority of learning is done by 50n (20,000) steps of interaction and the random agent is overtaken almost immediatly. \n",
    "\n",
    "(C) In five sentences or less, explain the key aspects of your implementation. How many state-action pairs do you represent in your Q-table? Describe and justify your settings of $\\alpha$ and $\\epsilon$. Are there any things you tried out that are not in your final implementation?\n",
    "\n",
    "In my implementation of q learning, a random npc move is made at the start of the game, then until the game ends a player move is chosen using an epsilon greedy method, then if the game is not finished the npc plays again. After this the state action pair of the initial board state and player action is updated as per the q learning algorithm, taking the previous value, reward and maximum potential q value of the new state into account (this process is repeated for each episode of 400 moves).\n",
    "\n",
    "Alpha dictates how much you value the most recent experince as opposed to the past q value, I found the optimal value to be 0.1 as anything more would be more likely to disregard past experinces given time, since the game is finite and fairly small I have used a gamma value of 1.\n",
    "\n",
    "Epsilon is the chance of picking a random move as opposed to the optimal one, initially I was going to vary epsilon but settled on a fixed value of 0.1, any higher and it picked randomly too often and any lower and it didn't experiment enough to learn efficiently.\n",
    "\n",
    "My Q-table is represented as a dictionary indexed by a grid/action tuple, it contains state action pairs for every state the game enters during play, if it searches for a state value pair not in the q table it adds it as 0, as such it stores values for every action for each state even if those moves have not been chosen yet.\n",
    "\n",
    "\n",
    "(D) In the cell below, make it possible for us to produce from scratch a learning curve similar to Plot 1 but for a single agent, for a $k$ value of your own choosing. You do not need to include the baseline for random play.  This code should run in less than 30 seconds (ours runs in 2 seconds). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e65915a61d304027e4fbd2e714c4beba",
     "grade": true,
     "grade_id": "cell-e0e01e05236aee45",
     "locked": false,
     "points": 40,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### This cell should produce from scratch a plot showing a learning curve for a single agent.\n",
    "\n",
    "#####################################################COPY PASTED FROM PART 1 ###################################\n",
    "### Write all your code for Part 1 within or above this cell. \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import connect\n",
    "import numpy as np\n",
    "\n",
    "#returns the maximum q value achiveable from the current state\n",
    "def max_next_item(q_table, grid):\n",
    "    table = []\n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        #spots that are in use are assigned a very low value so they are never picked \n",
    "        if (env.grid[2,i] == \"x\" or env.grid[2,i] == \"o\"):\n",
    "            q_table[grid, i] = 0\n",
    "            table += [-10000]\n",
    "        elif ((grid, i) in q_table):\n",
    "            table += [q_table[grid, i]]\n",
    "        else:\n",
    "            q_table[grid, i] = 0\n",
    "            table += [0]\n",
    "            \n",
    "    maximum = max(table)\n",
    "    return maximum\n",
    "\n",
    "\n",
    "#returns the best possible move from current state\n",
    "def max_next(q_table, grid):\n",
    "    table = []\n",
    "    for i in range(0, 5):\n",
    "        if (env.grid[2,i] == \"x\" or env.grid[2,i] == \"o\"):\n",
    "            q_table[grid, i] = 0\n",
    "            table += [-10000]\n",
    "        elif ((grid, i) in q_table):\n",
    "            table += [q_table[grid, i]]\n",
    "        else:\n",
    "            q_table[grid, i] = 0\n",
    "            table += [0]\n",
    "            \n",
    "    maximum = max(table)\n",
    "    max_moves = []\n",
    "    for i in range(0, 5):\n",
    "        if (table[i] == maximum):\n",
    "            max_moves += [i]\n",
    "    \n",
    "    #if there are multiple moves with the same q value pick randomly between them to avoid bias\n",
    "    if (len(max_moves) == 1):\n",
    "        return max_moves[0]\n",
    "    else:\n",
    "        random_move = random.randint(0,len(max_moves)-1)\n",
    "        return max_moves[random_move]\n",
    "\n",
    "    \n",
    "#converts grid to a string that can be used as a dictionary index\n",
    "def convert_grid_to_state(current_grid):\n",
    "    state = \"\"\n",
    "    for i in range(0, 3):\n",
    "        for j in range(0, 5):\n",
    "            if (current_grid[i][j] == \"x\"):\n",
    "                state+=\"x\"\n",
    "            elif (current_grid[i][j] == \"o\"):\n",
    "                state+=\"o\"\n",
    "            else:\n",
    "                state+=\"y\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "def plot(wins, random_wins):\n",
    "    %matplotlib inline\n",
    "    x = np.arange(0,len(wins))\n",
    "    x = x*400\n",
    "    y = np.array(wins)\n",
    "    y2 = np.array(random_wins)\n",
    "    fig, ax1 = plt.subplots(nrows = 1, ncols = 1 )\n",
    "    ax1.plot(x,y2, color = \"red\", label=\"random agent\")\n",
    "    ax1.plot(x,y, color = \"blue\", label=\"learning agent\")\n",
    "    ax1.set_xlabel('n')\n",
    "    ax1.set_ylabel('average agent score')\n",
    "    ax1.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#decides whether to pick a random move or an optimal one\n",
    "def pickMove(q_table):\n",
    "    epsilon = 0.1\n",
    "    random_action_chance = random.randint(0,10)\n",
    "    #find action with biggest q value\n",
    "    move = max_next(q_table, convert_grid_to_state(env.grid))\n",
    "\n",
    "    if (random_action_chance <= epsilon*10):\n",
    "        #choose random action\n",
    "        random_move = random.randint(0,4)\n",
    "        while (env.grid[2,move] == \"x\" or env.grid[2,move] == \"o\"):\n",
    "            move = random.randint(0,4)\n",
    "            \n",
    "    return move\n",
    "\n",
    "#plays n games and updates q table\n",
    "def train_for_n_steps(q_table):\n",
    "    n = 400\n",
    "    alpha = 0.1\n",
    "    moves = 0\n",
    "    gamma = 1\n",
    "\n",
    "    while (moves <= n):\n",
    "        env.reset()\n",
    "        #radnom npc move\n",
    "        reward, game_over = env.act(random.randint(0,4))\n",
    "\n",
    "        while (game_over == False):\n",
    "            move = pickMove(q_table)\n",
    "            grid = convert_grid_to_state(env.grid)\n",
    "            prev = q_table[grid, move]\n",
    "            previous_grid = grid\n",
    "            reward, game_over = env.act(move)\n",
    "            #random npc move\n",
    "            if (game_over == False):\n",
    "                #prevents npc making invalid move\n",
    "                npc_move = random.randint(0,4)\n",
    "                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\n",
    "                    npc_move = random.randint(0,4)\n",
    "                reward, game_over = env.act(npc_move)\n",
    "\n",
    "            #update q table for game state and action chosen\n",
    "            update_value = prev + alpha * (reward + gamma * (max_next_item(q_table, convert_grid_to_state(env.grid)) - prev))\n",
    "            q_table[previous_grid, move] = update_value\n",
    "            moves = moves + 1\n",
    "            if (moves == n):\n",
    "                break\n",
    "            \n",
    "    return(q_table)\n",
    "        \n",
    "#play 10 games with the current q table and don't change it        \n",
    "def play_m_games(q_table):\n",
    "    reward = 0\n",
    "    m = 9\n",
    "    score = 0\n",
    "    gamesPlayed = 0\n",
    "    while (gamesPlayed <= m):\n",
    "        score = score + reward\n",
    "        env.reset()\n",
    "        reward, game_over = env.act(random.randint(0,4))\n",
    "        gamesPlayed = gamesPlayed + 1\n",
    "        while (game_over == False):\n",
    "            move = pickMove(q_table)\n",
    "            reward, game_over = env.act(move)\n",
    "            if (game_over == False):\n",
    "                #prevents npc making invalid move\n",
    "                npc_move = random.randint(0,4)\n",
    "                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\n",
    "                    npc_move = random.randint(0,4)\n",
    "                reward, game_over = env.act(npc_move)\n",
    "    \n",
    "    score = score + reward\n",
    "    return score\n",
    "\n",
    "#play 10 games making random moves for player and npc\n",
    "def play_m_random_games():\n",
    "    reward = 0\n",
    "    m = 9\n",
    "    score = 0\n",
    "    gamesPlayed = 0\n",
    "    while (gamesPlayed <= m):\n",
    "        score = score + reward\n",
    "        env.reset()\n",
    "        reward, game_over = env.act(random.randint(0,4))\n",
    "        gamesPlayed = gamesPlayed + 1\n",
    "        while (game_over == False):\n",
    "            move = random.randint(0,4)\n",
    "            while (env.grid[2,move] == \"x\" or env.grid[2,move] == \"o\"):\n",
    "                move = random.randint(0,4)\n",
    "                \n",
    "            reward, game_over = env.act(move)\n",
    "            \n",
    "            if (game_over == False):\n",
    "                npc_move = random.randint(0,4)\n",
    "                while (env.grid[2,npc_move] == \"x\" or env.grid[2,npc_move] == \"o\"):\n",
    "                    npc_move = random.randint(0,4)\n",
    "\n",
    "                reward, game_over = env.act(npc_move)\n",
    "                \n",
    "    score = score + reward\n",
    "    #print(score)\n",
    "    return score\n",
    "            \n",
    "    \n",
    "#Main code body    \n",
    "env = connect.Connect(starting_player='x', verbose=False)\n",
    "\n",
    "\n",
    "#######################################REPEAT CODE ENDS HERE###############################################################\n",
    "\n",
    "q_table = {}\n",
    "wins = []\n",
    "for k in range(0,100):\n",
    "    q_table = train_for_n_steps(q_table)\n",
    "    wins += [play_m_games(q_table)]\n",
    "\n",
    "%matplotlib inline\n",
    "x = np.arange(0,len(wins))\n",
    "x = x\n",
    "y = np.array(wins)\n",
    "y2 = np.array(wins)\n",
    "fig, ax1 = plt.subplots(nrows = 1, ncols = 1 )\n",
    "ax1.plot(x,y, color = \"blue\", label=\"learning agent\")\n",
    "ax1.set_xlabel('n')\n",
    "ax1.set_ylabel('agent wins')\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+# IMPORTANT: How to submit.\n",
    "\n",
    "If any of the following instructions is not clear, please ask your tutors well ahead of the submission deadline.\n",
    "\n",
    "### Before you submit\n",
    "- We will not be able to mark your coursework if it takes more than 1 minutes to execute your entire notebook. That is, comment out (but do not delete) the code that you used to produce Plot 1 (i.e., learning curve averaged across many agents). Do **not** comment out the code that you use to produce a learning curve for a single agent (Exercise D).\n",
    "- Restart the kernel (_Kernel $\\rightarrow$ Restart & Run All_) and make sure that you can run all cells from top to bottom without any errors.\n",
    "- Make sure that your code is written in Python 3 (and not in Python 2!). You can check the Python version of the current session in the top-right corner below the Python logo.\n",
    "\n",
    "### Submission file\n",
    "- Please upload to Moodle a .zip file (**not** `.rar`, `.7z`, or any other archive format) that contains the completed Jupyter notebook (`ai4_connect_three.ipynb`) as well as the pre-computed figure(s). \n",
    "- **If** you change the `connect.py` file or write your own version of the environment, include the corresponding file in your submission, but give it any other name than `connect.py`. If you do not change its name, it will be overwritten  and we won't be able to execute your code! Make sure that you import the correct module when you rename your file, for example, use `import myConnect` if your file is called `myConnect.py`.\n",
    "- Do not include any identifying information. Not in the code cells, not in the file names, nowhere! Marking is anonymous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
